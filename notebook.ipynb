{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "<h1 align='center'>\n",
    "DECODING WORKSHOP\n",
    "</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Jupyter notebook usage\n",
    "\n",
    "This notebook allows to run python code interactively. Each code cell can be edited and run separately.\n",
    "\n",
    "### Run a cell\n",
    "Click on the following cell to select it and press the `Run` button or use the shortcut `Shitf+Enter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click here and press Shift + Enter \n",
    "print('Hello World')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Variables scopes\n",
    "All variables and functions of the notebooks are shared. Be careful not to override already existing variables by mistake!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we declare a variable\n",
    "tuto_string = 'Initial string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print it\n",
    "print(tuto_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we override it\n",
    "tuto_string = 'Overriden string'\n",
    "print('now run the previous cell again to print the string')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "**‚Üí The order in which you run the cells matters**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Python basics\n",
    "\n",
    "Python is a free and open source programming language that aims at being readable and fast to program with.\n",
    "\n",
    "A big strength of Python is its extensive standard library that provides convenient tools for usual tasks. It is also very easy to install or distribute modules with it. Many high quality modules have been created by the community, in particular we'll use a few targetting machine learning.\n",
    "\n",
    "You can skip this part if you're already familiar with Python üëç"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Math operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring variables\n",
    "a = 2\n",
    "b = 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum\n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiplication\n",
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# division\n",
    "a / b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer division\n",
    "5 // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# power (5^2)\n",
    "5 ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [1, 4.0, 5, 'hi', [0,1,2]]\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accessing the first element\n",
    "l[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accessing the last element\n",
    "l[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can access any element of a list in a list\n",
    "l[-1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accessing a slice\n",
    "l[1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# many objects have a length that you can get with len()\n",
    "len(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### For loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits = ['apple', 'banana', 'orange']\n",
    "\n",
    "for fruit in fruits:\n",
    "    print(fruit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's find the fruit names with an 'a'\n",
    "\n",
    "fruits = ['apple', 'banana', 'orange', 'kiwi']\n",
    "has_an_a = []\n",
    "   \n",
    "for fruit in fruits:\n",
    "    if 'a' in fruit:\n",
    "        has_an_a.append(fruit) # we can add an element to a list with list.append(element)\n",
    "   \n",
    "print(\"List of fruits with an 'a':\", has_an_a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_basket = ['apple', 'banana', 'lemon']\n",
    "\n",
    "my_favorite = 'lemon'\n",
    "\n",
    "for fruit in the_basket:\n",
    "    if fruit == my_favorite:\n",
    "        print(f\"There's a {my_favorite} left in the basket!\")\n",
    "    else:\n",
    "        print(f\"I don't like {fruit}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "5 < 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "'python' != 5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "'True' == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "4 <= 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addition(x,y):\n",
    "    return x + y\n",
    "\n",
    "res = addition(6,5)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters can be optional if they have a default value\n",
    "\n",
    "def print_passion(name='trains'):\n",
    "    print(f'I like {name}')\n",
    "    \n",
    "print_passion()\n",
    "print_passion(name='knitting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can return multiple values\n",
    "\n",
    "def stats(list_of_numbers):\n",
    "    return max(list_of_numbers), min(list_of_numbers) # min and max functions are built-in\n",
    "\n",
    "l = [3,6,1,8,12]\n",
    "\n",
    "maximum, minimum = stats(l)\n",
    "\n",
    "print(f'{maximum}, {minimum}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### Comprehensions\n",
    "\n",
    "Due to it's nature, Python is rather slow for large for loops, in particular when adding elements to a list with append. \n",
    "\n",
    "It is common to use comprehensions instead for both conciseness and optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "square_numbers = [i**2 for i in range(10)]\n",
    "\n",
    "print(square_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list comprehensions support conditions\n",
    "\n",
    "fruits = ['apple', 'banana', 'orange', 'kiwi']\n",
    "\n",
    "has_an_a = [fruit for fruit in fruits if 'a' in fruit]\n",
    "\n",
    "print(has_an_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "# TODO: Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "import numpy as np # library providing efficient array manipulation\n",
    "import sklearn # machine learning tools\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt # matlab-like plot library\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 22})\n",
    "\n",
    "from librosa.util import frame\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, balanced_accuracy_score, mean_squared_error\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from pyriemann.estimation import Covariances, XdawnCovariances\n",
    "from pyriemann.classification import MDM\n",
    "\n",
    "\n",
    "from scipy.signal import iirdesign, sosfilt, convolve, wiener\n",
    "\n",
    "import tp # load python module written for this project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "# Part 0 : Dataset\n",
    "\n",
    "In this workshop, we'll decode finger flexion from ECoG recordings. The data includes the recordings of the flexion of the subjects 5 fingers.\n",
    "\n",
    "More details here: https://www.bbci.de/competition/iv/desc_4.pdf\n",
    "\n",
    "### Download dataset\n",
    "\n",
    "Run the following cell to download the dataset. \n",
    "\n",
    "If it fails, please manually download and extract to `tp-decoding/data/bciciv/`:\n",
    "1. the training data: https://www.bbci.de/competition/download/competition_iv/BCICIV_4_mat.zip\n",
    "2. the evaluation data: https://www.bbci.de/competition/iv/results/ds4/true_labels.zip\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'data/bciciv'\n",
    "if not tp.dataset_exists(dataset_path):\n",
    "    tp.download_dataset(dataset_path)\n",
    "else:\n",
    "    print(f'dataset already exists in {dataset_path}') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tp.load_data(dataset_path)\n",
    "fs = 1000 # sampling frequency of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "### Explore dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in dataset:\n",
    "    print(len(subject.train_ecog))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_data = dataset[0] # first subject in dataset\n",
    "print(subject_data.train_fingers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data is stored in numpy arrays, you can get their size by accessing their 'shape' property\n",
    "\n",
    "subject_data.train_fingers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get first sample for all fingers, indexing starts at 0 /!\\\n",
    "\n",
    "subject_data.train_fingers[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get last sample for all fingers\n",
    "\n",
    "subject_data.train_fingers[-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 100 samples from index 1200 to 1299 for the 5th finger\n",
    "\n",
    "subject_data.train_fingers[1200:1300, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "### Plot the data with matplotlib\n",
    "You can find the documentation for the `plot` function here: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html\n",
    "\n",
    "For more information, you can check the [quick start guide](https://matplotlib.org/stable/users/explain/quick_start.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fingers = subject_data.train_fingers.shape[1]\n",
    "\n",
    "fig = plt.figure(figsize=(50,20))\n",
    "axes = fig.subplots(nrows=n_fingers, ncols=1, sharex=True)\n",
    "\n",
    "for i in range(n_fingers):\n",
    "    axes[i].plot(subject_data.train_fingers[:,i])\n",
    "    axes[i].set_ylabel(f'finger {i+1}')\n",
    "\n",
    "_ = fig.suptitle('Finger trajectories (train set)')\n",
    "\n",
    "# you can double clik on large plots to zoom in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_ratio = 40\n",
    "\n",
    "\n",
    "(n_times_train, n_fingers) = subject_data.train_fingers.shape\n",
    "n_channels = subject_data.train_ecog.shape[1]\n",
    "n_times_train = n_times_train//f_ratio\n",
    "threshold = 0.5\n",
    "n_times_test = subject_data.test_fingers.shape[0]//f_ratio\n",
    "\n",
    "win_size = 25\n",
    "win = 1/win_size*np.ones([win_size, 1])\n",
    "train_fingers_filtered = convolve(subject_data.train_fingers[::f_ratio], win, 'same')\n",
    "train_fingers_filtered = np.concatenate((train_fingers_filtered, threshold*np.ones([n_times_train, 1])), axis=1)\n",
    "y_train = np.argmax(train_fingers_filtered, axis=1)\n",
    "cue_train = np.zeros([n_times_train, n_fingers+1])\n",
    "for i in range(n_times_train):\n",
    "    cue_train[i, y_train[i]] = 1\n",
    "\n",
    "test_fingers_filtered = convolve(subject_data.test_fingers[::f_ratio], win, 'same')\n",
    "test_fingers_filtered = np.concatenate((test_fingers_filtered, threshold*np.ones([n_times_test, 1])), axis=1)\n",
    "y_test = np.argmax(test_fingers_filtered, axis=1)\n",
    "cue_test = np.zeros([n_times_test, n_fingers+1])\n",
    "for i in range(n_times_test):\n",
    "    cue_test[i, y_test[i]] = 1\n",
    "\n",
    "# Save the full data for future use\n",
    "y_train_full = y_train\n",
    "y_test_full = y_test\n",
    "\n",
    "fig = plt.figure(figsize=(150,5))\n",
    "ax = fig.subplots(3, 1, sharex=True)\n",
    "for i in range(n_fingers):\n",
    "    ax[0].plot(subject_data.train_fingers[::f_ratio, i])\n",
    "    ax[1].plot(train_fingers_filtered[:, i])\n",
    "    ax[2].plot(cue_train[:, i])\n",
    "ax[1].plot([0, n_times_train], [threshold, threshold], color='k')\n",
    "fig.suptitle('Finger trajectories (train set)')\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(150,5))\n",
    "ax = fig.subplots(3, 1, sharex=True)\n",
    "for i in range(n_fingers):\n",
    "    ax[0].plot(subject_data.test_fingers[::f_ratio, i])\n",
    "    ax[1].plot(test_fingers_filtered[:, i])\n",
    "    ax[2].plot(cue_test[:, i])\n",
    "ax[1].plot([0, n_times_test], [threshold, threshold], color='k')\n",
    "_ = fig.suptitle('Finger trajectories (test set)')\n",
    "\n",
    "# click on the figure to expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's inspect the ECoG channels\n",
    "\n",
    "# tp.plot_ecog(subject_data.train_ecog) # scroll on the figure to see all channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obviously there's something wrong, let's plot it again without problematic channels\n",
    "\n",
    "# exclude_channels = [54] # list the channels you'd like to remove\n",
    "\n",
    "# n_channels = subject_data.train_ecog.shape[1]\n",
    "# channel_selection = [i for i in range(n_channels) if i not in exclude_channels]\n",
    "\n",
    "# tp.plot_ecog(subject_data.train_ecog, channel_selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "# Part 1 : Dicrete decoding\n",
    "\n",
    "In this part we will be interested in decoding finger mouvement during tapping from ECOG data and in order to do so we will focus on start with feature extraction by extracting time-frequency features on every electrodes using short term fourier transfrom. \n",
    "\n",
    "After this we will get a first set of results that we will visualize using different metrics such as the confusion matrix, the accuracy and the balanced accuracy. \n",
    "\n",
    "Then we will have an example of the usefullness of a validation set in the case of lagged responses. \n",
    "\n",
    "Once the lag have been set, we will see that we can also get better results using spatial filters such as PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "## 1.1 Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "### Temporal filtering\n",
    "\n",
    "In this part we will focus on the filtering of the data using classical filtering function such as `iirdesign` and `sosfilt` from `scipy.signal` (https://docs.scipy.org/doc/scipy/reference/signal.html). \n",
    "It is pretty common in ECOG to filter in several frequency bands and use all the resulting features for the decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bands = [(1, 60), (60, 100), (100, 200)]\n",
    "# bands = [(1, 10)]\n",
    "# for i in range(1, 20):\n",
    "#     bands.append((i*10, (i+1)*10))\n",
    "filters = []\n",
    "\n",
    "bands = [(1, 10), (10,30), (30,50), (70,200)]\n",
    "n_filters = len(bands)\n",
    "\n",
    "for i_band, band in enumerate(bands):\n",
    "   # filters.append(iirdesign(band[0], band[1], 1, 60, analog=False, fs=fs, output='sos'))\n",
    "    filters.append(iirdesign(band, (band[0]*0.9, band[1]*1.1), 2, 20, analog=False, fs=fs, output='sos'))\n",
    "\n",
    "ecog_train = tp.common_average_reference(subject_data.train_ecog, axis=1)\n",
    "ecog_test = tp.common_average_reference(subject_data.test_ecog, axis=1)\n",
    "\n",
    "\n",
    "ecog_train = subject_data.train_ecog\n",
    "ecog_test = subject_data.test_ecog\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_ecog_normalized = scaler.fit_transform(ecog_train)\n",
    "test_ecog_normalized = scaler.transform(ecog_test)\n",
    "\n",
    "train_ecog_filtered = []\n",
    "test_ecog_filtered = []\n",
    "\n",
    "for filt in filters:\n",
    "    train_ecog_filtered.append(sosfilt(filt, train_ecog_normalized, axis=0))\n",
    "    test_ecog_filtered.append(sosfilt(filt, test_ecog_normalized, axis=0))\n",
    "\n",
    "fig = plt.figure(figsize=(50,15))\n",
    "ax = fig.subplots(nrows=4, ncols=1)\n",
    "\n",
    "ax[0].plot(train_ecog_filtered[0][:1000, 0])\n",
    "ax[1].plot(train_ecog_filtered[1][:1000, 0])\n",
    "ax[2].plot(train_ecog_filtered[2][:1000, 0])\n",
    "ax[3].plot(train_ecog_filtered[3][:1000, 0])\n",
    "\n",
    "ax[0].plot(train_ecog_normalized[:1000, 0])\n",
    "ax[1].plot(train_ecog_normalized[:1000, 0])\n",
    "ax[2].plot(train_ecog_normalized[:1000, 0])\n",
    "ax[3].plot(train_ecog_normalized[:1000, 0])\n",
    "\n",
    "fig.suptitle('Filtered data (train set)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "### Average power computation\n",
    "\n",
    "Once the signal has been filtered, you can extract the average power of the signal in those frequency band using the `power` and `mean` function from `numpy`. Note that the formula for the average power of a centered signal $x$ is \n",
    "\n",
    "\\begin{align}\n",
    "P(x) = \\frac{1}{N}\\sum_{t}x^2(t),\n",
    "\\end{align}\n",
    "\n",
    "with $N$ the number of elements in the window.  \n",
    "\n",
    "In this part we will extract power in 1s of signal every 40ms. In order to compute the frame to be averaged you can use the fonction `frame` from `librosa.util`(https://librosa.org/doc/latest/generated/librosa.util.frame.html) and the padding function `pad` from `numpy` because for the first frames you can't take the 1s of previous signal as it doesn't exist, therefore you can replace it with zeros.  \n",
    "\n",
    "For an easier buffering version, you can use the `tp.buffering` function that has been directly created for this hand on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_size = 1000\n",
    "n_buffer = 1;\n",
    "\n",
    "X_train_filtered = np.concatenate(train_ecog_filtered, axis=1)\n",
    "print(X_train_filtered.shape)\n",
    "X_train = tp.buffering_power(X_train_filtered, win_size, f_ratio, n_buffer)\n",
    "X_train_full = X_train\n",
    "print(X_train_full.shape)\n",
    "\n",
    "X_test_filtered = np.concatenate(test_ecog_filtered, axis=1)\n",
    "X_test = tp.buffering_power(X_test_filtered, win_size, f_ratio, n_buffer)\n",
    "X_test_full = X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "### Set if we use the idle or not\n",
    "\n",
    "In order to make things easier you can chose to select only the fingers and not the idle task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "use_idle_data = True\n",
    "if not use_idle_data:\n",
    "    X_train = X_train_full[y_train_full != 5]\n",
    "    X_test = X_test_full[y_test_full != 5]\n",
    "    y_train = y_train_full[y_train_full != 5]\n",
    "    y_test = y_test_full[y_test_full != 5]\n",
    "    n_times_train = y_train.shape[0]\n",
    "    n_times_test = y_test.shape[0]\n",
    "else:\n",
    "    X_train = X_train_full\n",
    "    X_test = X_test_full\n",
    "    y_train = y_train_full\n",
    "    y_test = y_test_full\n",
    "    n_times_train = y_train.shape[0]\n",
    "    n_times_test = y_test.shape[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "## 1.2 Classification\n",
    "\n",
    "### The classifier\n",
    "\n",
    "A classifier is an algorithm that aims to classify given inputs (here the power in given frequency bands) into outputs (here our 5 possible fingers + the idle class where the user does nothing). \n",
    "\n",
    "In this particular example we will use either the `LinearDiscriminantAnalysis` (imported here as `LDA`) from `sklearn.discriminant_analysis` or the `LinearSVC` from `sklearn.svm`. \n",
    "See their documentation here : \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC\n",
    "\n",
    "In the sklearn API, all classes are using the `fit` function to train the algorithm and the `predict` function to get the predictions for the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "### Fit the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = LDA()\n",
    "\n",
    "estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "### Get the predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = estimator.predict(X_test)\n",
    "y_proba = estimator.predict_proba(X_test)\n",
    "\n",
    "cue_pred = np.zeros([n_times_test, n_fingers+1])\n",
    "for i in range(n_times_test):\n",
    "    cue_pred[i, y_pred[i]] = 1\n",
    "\n",
    "fig = plt.figure(figsize=(150,5))\n",
    "ax = fig.subplots(3, 1)\n",
    "\n",
    "cue_test = np.zeros([n_times_test, n_fingers+1])\n",
    "for i in range(n_times_test):\n",
    "    cue_test[i, y_test[i]] = 1\n",
    "\n",
    "for i in range(n_fingers):\n",
    "    ax[0].plot(cue_test[:, i])\n",
    "    ax[1].plot(y_proba[:, i])\n",
    "    ax[2].plot(cue_pred[:, i])\n",
    "fig.suptitle('Finger trajectories (test set)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "## 1.3 Confusion matrix and metrics\n",
    "\n",
    "Once you get your predictions you usually want to evaluate the performance of your decoder. In order to do so, a lot of different metrics have been developped with time. Most of the are using what is called a confusion matrix in order to be computed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "### The confusion matrix\n",
    "\n",
    "The confusion matrix represents how well our classifier is performing. The rows are the true labels we are expected to obtain and the columns are the labels our classifier actually gives us. \n",
    "\n",
    "In order to plot the confusion matrix, you can use the `from_predictions` function from the `ConfusionMatrixDisplay` class of `sklearn.metrics`.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(100,100))\n",
    "disp = ConfusionMatrixDisplay.from_predictions(y_test, y_pred) # , text_kw={'fontsize': 10}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "### The accuracy\n",
    "\n",
    "A good way to consider how well the classifer is performing using the confusion matrix is the accuracy. The accuracy is equal to the total number of well predicted labels divided by the total number of labels. In the confusion matrix it is equal to the sum of the diagonal divided by the sum of all the elements of the matrix. \n",
    "\n",
    "You can get the accuracy using the `accuracy_score` function from `sklearn.metrics`. \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy is {:.2f}%. Chance is at {:.2f}%\".format(100*np.mean(acc), 100/np.unique(y_test).shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "### The balanced accuracy\n",
    "\n",
    "In some cases there are more labels from some classes than others, for example here the finger 2 and 5 are less represented than others so their accuracy accounts for less than the others when we compute the accuracy metric. A good way to consider every labels with the same weight is to compute the balanced accuracy. It gives the accuracy for each class reweighted so that every class accounts for as much as the others. \n",
    "\n",
    "It is computed using the `balanced_accuracy_score` function from `sklearn.metrics`.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html#sklearn-metrics-balanced-accuracy-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_fingers+1):\n",
    "    print(\"There are {} labels in test for class {}\".format(np.sum(y_test==i), i+1))\n",
    "\n",
    "plt.hist(y_test, bins=n_fingers+1, rwidth=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = balanced_accuracy_score(y_test, y_pred)\n",
    "print(\"Average score is {:.2f}%. Chance is at {:.2f}%\".format(100*np.mean(acc), 100/np.unique(y_test).shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "## 1.4 The validation set\n",
    "\n",
    "Sometimes algorithms and methods need to set a specific parameter to get better results. In order to do so, we usually use what we call a validation set. This set is usually a subdivision of the training set that works as a validation for the parameter we are trying to optimize. In the next example, we will try to find the best lag between the input and the output.\n",
    "\n",
    "### Use the validation to compute a lag value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = [i for i in range(-26, 26, 2)] \n",
    "X_train_val, X_val, y_train_val, y_val = train_test_split(X_train, y_train, test_size=0.33, random_state=13, shuffle=False)\n",
    "lagged_accuracies = []\n",
    "n_times_train_val = X_train_val.shape[0]\n",
    "n_times_val = X_val.shape[0]\n",
    "for i_lag, lag in enumerate(lags):\n",
    "    X_train_lagged = X_train_val[max(-lag, 0):min(n_times_train_val-lag, n_times_train_val)]\n",
    "    y_train_lagged = y_train_val[max(lag, 0):min(n_times_train_val+lag, n_times_train_val)]\n",
    "    X_val_lagged = X_val[max(-lag, 0):min(n_times_val-lag, n_times_val)]\n",
    "    y_val_lagged = y_val[max(lag, 0):min(n_times_val+lag, n_times_val)]\n",
    "    \n",
    "    estimator.fit(X_train_lagged, y_train_lagged)\n",
    "    y_pred_val = estimator.predict(X_val_lagged)\n",
    "    lagged_accuracies.append(accuracy_score(y_val_lagged, y_pred_val))\n",
    "    print(\"Lag {}/{} ({:.2f}s) -> Accuracy = {:.2f}%\".format(i_lag+1, len(lags), lag/25, lagged_accuracies[-1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {},
   "source": [
    "### Compute the best lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(-1, 1, len(lagged_accuracies)), lagged_accuracies)\n",
    "best_lag = lags[np.argmax(lagged_accuracies)]\n",
    "print(\"The best lag is {}. This lag correspond to {:.2f}s\".format(best_lag, best_lag/25))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "### Compute lagged X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lagged = X_train[max(-best_lag, 0):min(n_times_train-best_lag, n_times_train)]\n",
    "y_train_lagged = y_train[max(best_lag, 0):min(n_times_train+best_lag, n_times_train)]\n",
    "n_times_train_lagged = n_times_train-abs(best_lag)\n",
    "\n",
    "X_test_lagged = X_test[max(-best_lag, 0):min(n_times_test-best_lag, n_times_test)]\n",
    "y_test_lagged = y_test[max(best_lag, 0):min(n_times_test+best_lag, n_times_test)]\n",
    "n_times_test_lagged = n_times_test-abs(best_lag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "### Train the estimator and decode testing data with lagged estimates of X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(X_train_lagged, y_train_lagged)\n",
    "y_pred = estimator.predict(X_test_lagged)\n",
    "y_proba = estimator.predict_proba(X_test_lagged)\n",
    "\n",
    "cue_pred = np.zeros([n_times_test_lagged, n_fingers+1])\n",
    "for i in range(n_times_test_lagged):\n",
    "    cue_pred[i, y_pred[i]] = 1\n",
    "\n",
    "cue_test = np.zeros([n_times_test_lagged, n_fingers+1])\n",
    "for i in range(n_times_test_lagged):\n",
    "    cue_test[i, y_test[i]] = 1\n",
    "\n",
    "fig = plt.figure(figsize=(150,5))\n",
    "ax = fig.subplots(3, 1)\n",
    "for i in range(n_fingers):\n",
    "    ax[0].plot(cue_test[:, i])\n",
    "    ax[1].plot(y_proba[:, i])\n",
    "    ax[2].plot(cue_pred[:, i])\n",
    "fig.suptitle('Finger trajectories (test set)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90",
   "metadata": {},
   "source": [
    "### Compute metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(100,100))\n",
    "disp = ConfusionMatrixDisplay.from_predictions(y_test_lagged, y_pred)\n",
    "\n",
    "acc = accuracy_score(y_test_lagged, y_pred)\n",
    "print(\"Accuracy is {:.2f}%. Chance is at {:.2f}%\".format(100*np.mean(acc), 100/np.unique(y_test).shape[0]))\n",
    "\n",
    "acc = balanced_accuracy_score(y_test_lagged, y_pred)\n",
    "print(\"Balanced accuracy is {:.2f}%. Chance is at {:.2f}%\".format(100*np.mean(acc), 100/np.unique(y_test).shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92",
   "metadata": {},
   "source": [
    "## 1.5 Feature selection and dimensionality reduction\n",
    "\n",
    "In some cases in machine learning there are so much features that the model cannot correctly learn for the training data. In these cases, it is very common to compute dimensionnality reduction or spatial filters in order to keep only the features of interest for decoding. In this example we will use a Principal Components Analysis (the `PCA` class from `sklearn.decomposition`) and compute patterns for each finger in order to improve the results.\n",
    "\n",
    "In `sklearn`, the classes that transform data without predicting are called transformers and use the functions `fit` and `transform` to process data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca = []\n",
    "X_test_pca = []\n",
    "for i in range(n_fingers):\n",
    "    print(\"Computing PCA patterns for finger {}...\".format(i+1))\n",
    "    s_filt = PCA(60)\n",
    "    s_filt.fit(X_train_lagged[y_train_lagged==i])\n",
    "    X_train_pca.append(s_filt.transform(X_train_lagged))\n",
    "    X_test_pca.append(s_filt.transform(X_test_lagged))\n",
    "\n",
    "X_train_pca = np.concatenate(X_train_pca, axis=1)\n",
    "X_test_pca = np.concatenate(X_test_pca, axis=1)\n",
    "\n",
    "estimator.fit(X_train_pca, y_train_lagged)\n",
    "y_pred_pca = estimator.predict(X_test_pca)\n",
    "y_proba_pca = estimator.predict_proba(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "cue_pred = np.zeros([n_times_test_lagged, n_fingers+1])\n",
    "for i in range(n_times_test_lagged):\n",
    "    cue_pred[i, y_pred_pca[i]] = 1\n",
    "\n",
    "cue_test = np.zeros([n_times_test_lagged, n_fingers+1])\n",
    "for i in range(n_times_test_lagged):\n",
    "    cue_test[i, y_test[i]] = 1\n",
    "\n",
    "fig = plt.figure(figsize=(150,5))\n",
    "ax = fig.subplots(3, 1)\n",
    "for i in range(n_fingers):\n",
    "    ax[0].plot(cue_test[:, i])\n",
    "    ax[1].plot(y_proba_pca[:, i])\n",
    "    ax[2].plot(cue_pred[:, i])\n",
    "fig.suptitle('Finger trajectories (test set)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(100,100))\n",
    "disp = ConfusionMatrixDisplay.from_predictions(y_test_lagged, y_pred_pca)\n",
    "\n",
    "\n",
    "acc = accuracy_score(y_test_lagged, y_pred_pca)\n",
    "print(\"Accuracy is {:.2f}%. Chance is at {:.2f}%\".format(100*np.mean(acc), 100/np.unique(y_test).shape[0]))\n",
    "\n",
    "acc = balanced_accuracy_score(y_test_lagged, y_pred_pca)\n",
    "print(\"Balanced accuracy is {:.2f}%. Chance is at {:.2f}%\".format(100*np.mean(acc), 100/np.unique(y_test).shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96",
   "metadata": {},
   "source": [
    "## 1.6 Get a more stable output (optional)\n",
    "\n",
    "On decoders that are to be used by humans, it is important to get a more stable output so that it doesn't radically change every 40ms for example. This does not always improve classification but it can improve the feeling of control for the user with the side effect of adding a bit of lag. \n",
    "\n",
    "In this way, we can get the output probability of the decoder using the `predict_proba` function and smooth it using a convolution with a window. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_size = 25\n",
    "win = np.expand_dims(scipy.signal.windows.hamming(win_size), 1)\n",
    "y_proba_stable = convolve(y_proba_pca, win, 'same')\n",
    "y_pred_stable = np.argmax(y_proba_stable, axis=1)\n",
    "\n",
    "cue_pred = np.zeros([n_times_test_lagged, n_fingers+1])\n",
    "for i in range(n_times_test_lagged):\n",
    "    cue_pred[i, y_pred_stable[i]] = 1\n",
    "\n",
    "cue_test = np.zeros([n_times_test_lagged, n_fingers+1])\n",
    "for i in range(n_times_test_lagged):\n",
    "    cue_test[i, y_test[i]] = 1\n",
    "\n",
    "fig = plt.figure(figsize=(150,5))\n",
    "ax = fig.subplots(3, 1)\n",
    "for i in range(n_fingers):\n",
    "    ax[0].plot(cue_test[:, i])\n",
    "    ax[1].plot(y_proba_stable[:, i])\n",
    "    ax[2].plot(cue_pred[:, i])\n",
    "fig.suptitle('Finger trajectories (test set)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "### Get metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(100,100))\n",
    "disp = ConfusionMatrixDisplay.from_predictions(y_test_lagged, y_pred_stable)\n",
    "\n",
    "acc = accuracy_score(y_test_lagged, y_pred_stable)\n",
    "print(\"Accuracy is {:.2f}%. Chance is at {:.2f}%\".format(100*np.mean(acc), 100/np.unique(y_test).shape[0]))\n",
    "\n",
    "acc = balanced_accuracy_score(y_test_lagged, y_pred_stable)\n",
    "print(\"Balanced accuracy is {:.2f}%. Chance is at {:.2f}%\".format(100*np.mean(acc), 100/6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {},
   "source": [
    "### Riemannian Geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_buffer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_filtered = np.concatenate(train_ecog_filtered, axis=1)\n",
    "X_train_filtered = train_ecog_filtered[1]\n",
    "win_size = 1000\n",
    "n_channels = X_train_filtered.shape[1]\n",
    "\n",
    "X_train_buffer = np.pad(X_train_filtered, [(win_size-f_ratio, 0), (0, 0)])\n",
    "X_train_buffer = frame(X_train_buffer, frame_length=win_size, hop_length=f_ratio, axis=0)\n",
    "n_trials = X_train_buffer.shape[0]\n",
    "X_train_buffer = np.transpose(X_train_buffer.reshape(n_trials, win_size, n_channels), (0, 2, 1))\n",
    "\n",
    "\n",
    "X_test_filtered = np.concatenate(test_ecog_filtered, axis=1)\n",
    "X_test_filtered = test_ecog_filtered[1]\n",
    "X_test_buffer = np.pad(X_test_filtered, [(win_size-f_ratio, 0), (0, 0)])\n",
    "X_test_buffer = frame(X_test_buffer, frame_length=win_size, hop_length=f_ratio, axis=0)\n",
    "n_trials_test = X_test_buffer.shape[0]\n",
    "X_test_buffer = np.transpose(X_test_buffer.reshape(n_trials_test, win_size, n_channels), (0, 2, 1))\n",
    "\n",
    "\n",
    "X_train_lagged = X_train_buffer[max(-best_lag, 0):min(n_times_train-best_lag, n_times_train)]\n",
    "y_train_lagged = y_train[max(best_lag, 0):min(n_times_train+best_lag, n_times_train)]\n",
    "n_times_train_lagged = n_times_train-abs(best_lag)\n",
    "\n",
    "X_test_lagged = X_test_buffer[max(-best_lag, 0):min(n_times_test-best_lag, n_times_test)]\n",
    "y_test_lagged = y_test[max(best_lag, 0):min(n_times_test+best_lag, n_times_test)]\n",
    "n_times_test_lagged = n_times_test-abs(best_lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cov = XdawnCovariances(estimator=\"oas\")\n",
    "\n",
    "# X_train_cov = cov.fit_transform(X_train_buffer, y_train)\n",
    "# X_test_cov = cov.transform(X_test_buffer)\n",
    "# print(X_train_cov.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator = MDM()\n",
    "# estimator.fit(X_train_cov, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = estimator.predict(X_test_cov)\n",
    "# y_proba = estimator.predict_proba(X_test_cov)\n",
    "\n",
    "# cue_pred = np.zeros([n_times_test, n_fingers+1])\n",
    "# for i in range(n_times_test):\n",
    "#     cue_pred[i, y_pred[i]] = 1\n",
    "\n",
    "# fig = plt.figure(figsize=(150,5))\n",
    "# ax = fig.subplots(3, 1)\n",
    "\n",
    "# cue_test = np.zeros([n_times_test, n_fingers+1])\n",
    "# for i in range(n_times_test):\n",
    "#     cue_test[i, y_test[i]] = 1\n",
    "\n",
    "# for i in range(n_fingers):\n",
    "#     ax[0].plot(cue_test[:, i])\n",
    "#     ax[1].plot(y_proba[:, i])\n",
    "#     ax[2].plot(cue_pred[:, i])\n",
    "# fig.suptitle('Finger trajectories (test set)')\n",
    "\n",
    "\n",
    "# acc = accuracy_score(y_test, y_pred)\n",
    "# print(\"Accuracy is {:.2f}%. Chance is at {:.2f}%\".format(100*np.mean(acc), 100/np.unique(y_test).shape[0]))\n",
    "\n",
    "# acc = balanced_accuracy_score(y_test, y_pred)\n",
    "# print(\"Balanced accuracy is {:.2f}%. Chance is at {:.2f}%\".format(100*np.mean(acc), 100/np.unique(y_test).shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "107",
   "metadata": {},
   "source": [
    "# Part 2 : Continuous decoding\n",
    "\n",
    "In this part we will be interested in decoding precise hand movements of a monkey from ECOG data and in order to do so we will procede just as in the Part 1 with\n",
    "- Pre-processing - where we will see artefact removal\n",
    "- Feature extraction - by extracting time-frequency features on every electrodes\n",
    "\n",
    "After this first part we will focus on regression with\n",
    "- Regression using a linear model \n",
    "- Regression using Ridge regularization to prevent overfitting\n",
    "- Cross validation in order to ensure the robustness of our testing procedure\n",
    "\n",
    "\n",
    "Finally we will evaluate the model performances using mean square error and correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "import numpy as np # library providing efficient array manipulation\n",
    "import sklearn # machine learning tools\n",
    "import matplotlib.pyplot as plt # matlab-like plot library\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 22})\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from scipy.signal import iirdesign, sosfilt, convolve, wiener\n",
    "\n",
    "import tp # load python module written for this project\n",
    "\n",
    "\n",
    "\n",
    "# Download dataset\n",
    "dataset_path = 'data/bciciv'\n",
    "if not tp.dataset_exists(dataset_path):\n",
    "    tp.download_dataset(dataset_path)\n",
    "else:\n",
    "    print(f'dataset already exists in {dataset_path}') \n",
    "\n",
    "\n",
    "# Load dataset\n",
    "dataset = tp.load_data(dataset_path)\n",
    "fs = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109",
   "metadata": {},
   "source": [
    "## 2.1 Data pre-processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111",
   "metadata": {},
   "source": [
    "### Channel selection\n",
    "\n",
    "It is not uncommon for recordings to have some noisy channels. For example, some electrodes might not properly contact the cortex, or might be faulty.\n",
    "\n",
    "After visual inspections of the electrodes, we can manually remove them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_channels = [54] # list the channels you'd like to remove\n",
    "\n",
    "n_channels = subject.train_ecog.shape[1]\n",
    "channel_selection = [i for i in range(n_channels) if i not in exclude_channels]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113",
   "metadata": {},
   "source": [
    "### Artefact removal \n",
    "\n",
    "Some noises are commonly shared between channels, for example line noise (50Hz or 60Hz and its harmonics). A simple method to remove such artefacts is the common average reference (CAR). It consists in computing the average of each channels at a given time step, and substract it from all channels at this time step:\n",
    "\n",
    "\\begin{align}\n",
    "ECoG(channel,t) = ECoG(channel,t) - \\frac{1}{N} \\sum_{c=0}^{N-1} ECoG(c,t)\n",
    "\\end{align}\n",
    "\n",
    "In order to improve the robustness of this method, it is recommended to first exclude channels that may be faulty. It is also possible to use the median instead of the mean, or apply the CAR only on neighbouring channels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common average reference\n",
    "\n",
    "ecog_train = tp.common_average_reference(subject.train_ecog[:,channel_selection], axis=1)\n",
    "ecog_test = tp.common_average_reference(subject.test_ecog[:,channel_selection], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ECoG with CAR vs without\n",
    "\n",
    "selection = [0]\n",
    "\n",
    "tp.plot_ecog(subject.train_ecog[1000:2000], selection) # without\n",
    "tp.plot_ecog(ecog_train[1000:2000], selection) # with\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = [(1, 10), (10,30), (30,50), (70,200)]\n",
    "\n",
    "band_filters = tp.compute_band_filters(bands)\n",
    "\n",
    "# filter\n",
    "X_train = np.array([sosfilt(filt, ecog_train, axis=0) for filt in band_filters])\n",
    "X_test = np.array([sosfilt(filt, ecog_test, axis=0) for filt in band_filters])\n",
    "\n",
    "Y_train = np.array(subject.train_fingers)\n",
    "Y_test = np.array(subject.test_fingers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.train_fingers.shape\n",
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot finger positions\n",
    "\n",
    "finger = 0 # pick a finger to plot\n",
    "\n",
    "fig = plt.figure(figsize=(60,15))\n",
    "\n",
    "ax = fig.subplots(1, 1)\n",
    "\n",
    "ax = plt.plot(subject.train_fingers[:,finger])\n",
    "ax = plt.plot(Y_train[:,finger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(50,20))\n",
    "ax = fig.subplots(nrows=6, ncols=1)\n",
    "\n",
    "s = slice(1000,2000)\n",
    "\n",
    "ax[0].plot(X_train[0][s, 0])\n",
    "ax[1].plot(X_train[1][s, 0])\n",
    "ax[2].plot(X_train[2][s, 0])\n",
    "ax[3].plot(X_train[3][s, 0])\n",
    "ax[4].plot(np.sum(X_train[:,s, 0], axis=0))\n",
    "ax[5].plot(ecog_train[s,0] - np.sum(X_train[:,s, 0], axis=0))\n",
    "\n",
    "ax[0].plot(ecog_train[s, 0])\n",
    "ax[1].plot(ecog_train[s, 0])\n",
    "ax[2].plot(ecog_train[s, 0])\n",
    "ax[3].plot(ecog_train[s, 0])\n",
    "ax[4].plot(ecog_train[s, 0])\n",
    "ax[5].plot(ecog_train[s, 0])\n",
    "\n",
    "_ = fig.suptitle('Filtered data (train set)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122",
   "metadata": {},
   "source": [
    "#### Reshape the ECoG signal \n",
    "1. to a shape of (samples, electrodes, frequency bands)\n",
    "2. to a shape of (samples, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.moveaxis(X_train, 0, -1)\n",
    "X_test = np.moveaxis(X_test, 0, -1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125",
   "metadata": {},
   "source": [
    "#### Compute the ECoG power of each feature\n",
    "effectively computing it for each frequency band of each electrode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy to avoid recomputing everything everytime\n",
    "\n",
    "X_train_save = np.array(X_train)\n",
    "X_test_save = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127",
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAME_LENGTH = 200 # length of the ECoG window on which we compute the signal's power \n",
    "HOP = 40 # we downsample the signal to 25Hz, the original frame rate of the glove\n",
    "BUFFERS = 4 # for each timestep t we concatenate features from timestep t to t-BUFFERS+1 included\n",
    "\n",
    "X_train = tp.buffering_power(X_train, FRAME_LENGTH, HOP, BUFFERS)\n",
    "X_test = tp.buffering_power(X_test, FRAME_LENGTH, HOP, BUFFERS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample Y to the same frame rate as X: 25 Hz\n",
    "\n",
    "from scipy.signal import decimate # downsample signals with antialiasing\n",
    "\n",
    "Y_train = decimate(Y_train, HOP, axis=0, zero_phase=True)\n",
    "Y_test = decimate(Y_test, HOP, axis=0, zero_phase=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130",
   "metadata": {},
   "source": [
    "## 2.2 Linear Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131",
   "metadata": {},
   "source": [
    "#### Validation\n",
    "\n",
    "We're splitting the train set in two to use the first part as train set and the second part as validation set.\n",
    "Indeed some methods require to optimize parameters and we need some unseen data to evaluate the best values for these parameters.\n",
    "\n",
    "The test set is left for final evaluation, once the best possible model has been optimized on train and validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val, X_val, Y_train_val, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=13, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133",
   "metadata": {},
   "source": [
    "#### Normalization\n",
    "We compute mean and standard deviation of the train set and use it to z-score the train, validation and test set. <br>\n",
    "\n",
    "\\begin{align}\n",
    "zscore(x) = \\frac{x-mean(x)}{std(x)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the sklearn implementation\n",
    "\n",
    "ecog_scaler = StandardScaler()\n",
    "X_train_val = ecog_scaler.fit_transform(X_train_val)\n",
    "X_val = ecog_scaler.transform(X_val)\n",
    "X_test = ecog_scaler.transform(X_test)\n",
    "\n",
    "finger_scaler = StandardScaler()\n",
    "Y_train_val = finger_scaler.fit_transform(Y_train_val)\n",
    "Y_val = finger_scaler.transform(Y_val)\n",
    "Y_test = finger_scaler.transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize lag \n",
    "for l in range(-4,5):\n",
    "    Y_train_lag = tp.lag(Y_train_val, l)\n",
    "    Y_val_lag = tp.lag(Y_val, l)\n",
    "    \n",
    "    reg = LinearRegression()\n",
    "    reg.fit(X_train_val, Y_train_lag)\n",
    "    print(f'lag: {l}')\n",
    "    print(f'\\t train: {reg.score(X_train_val, Y_train_lag)}')\n",
    "    print(f'\\t val: {reg.score(X_val, Y_val_lag)}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137",
   "metadata": {},
   "source": [
    "#### Built-in score evaluation\n",
    "\n",
    "The built-in [score()](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge.score) function of sckiti-learn models returns the coefficient of determination of the prediction.\n",
    "\n",
    "The coefficient of determination $R^2$ is defined as:\n",
    "\n",
    "\\begin{align}\n",
    "R^2 = (1 - \\frac{u}{v})\n",
    "\\end{align}\n",
    "\n",
    "where $u$ is the residual sum of squares `((y_true - y_pred)** 2).sum()` <br>\n",
    "and $v$ is the total sum of squares `((y_true - y_true.mean()) ** 2).sum()`\n",
    "\n",
    "The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). <br>\n",
    "A constant model that always predicts the expected value of $y$, disregarding the input features, would get\n",
    "a $R^2$ score of 0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settle with 0 lag\n",
    "\n",
    "for finger in range(Y_train_val.shape[1]):\n",
    "    reg = LinearRegression()\n",
    "    reg.fit(X_train_val, Y_train_val[:,finger])\n",
    "    print(f'Finger {finger}:')\n",
    "    print(f'\\t train: {reg.score(X_train_val, Y_train_val[:,finger])}')\n",
    "    print(f'\\t test:  {reg.score(X_val, Y_val[:,finger])}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions (blue) and ground truth (orange)\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train_val, Y_train_val)\n",
    "\n",
    "fig = plt.figure(figsize=(90,50))\n",
    "ax = fig.subplots(nrows=10, ncols=1)\n",
    "\n",
    "Y_train_predict = reg.predict(X_train_val)\n",
    "Y_test_predict = reg.predict(X_val)\n",
    "\n",
    "for finger in range(5):\n",
    "    ax[2*finger].plot(Y_train_predict[:,finger])\n",
    "    ax[2*finger].plot(Y_train_val[:,finger])\n",
    "    ax[2*finger].set_ylabel(f'finger {finger+1} (train)')\n",
    "    \n",
    "    ax[2*finger+1].plot(Y_test_predict[:,finger])\n",
    "    ax[2*finger+1].plot(Y_val[:,finger])\n",
    "    ax[2*finger+1].set_ylabel(f'finger {finger+1} (val)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140",
   "metadata": {},
   "source": [
    "#### Decoding speed instead of position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(X):\n",
    "    \"\"\" derivative by computing difference between samples n+1 and sample n-1 \"\"\"\n",
    "    return (tp.lag(X,1) - tp.lag(X,-1)) / 2\n",
    "\n",
    "Y_train_speed = derivative(Y_train_val)\n",
    "Y_val_speed = derivative(Y_val)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(200,10))\n",
    "ax = fig.subplots(nrows=1, ncols=1)\n",
    "\n",
    "# ax.plot(Y_train)\n",
    "_ = ax.plot(Y_train_speed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142",
   "metadata": {},
   "outputs": [],
   "source": [
    "for finger in range(Y_train.shape[1]):\n",
    "    reg = LinearRegression()\n",
    "    reg.fit(X_train_val, Y_train_speed[:,finger])\n",
    "    print(f'Finger {finger}:')\n",
    "    print(f'\\t train: {reg.score(X_train_val, Y_train_speed[:,finger])}')\n",
    "    print(f'\\t val:  {reg.score(X_val, Y_val_speed[:,finger])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(X_train_val, Y_train_speed)\n",
    "\n",
    "fig = plt.figure(figsize=(90,50))\n",
    "ax = fig.subplots(nrows=10, ncols=1)\n",
    "\n",
    "Y_train_predict = reg.predict(X_train_val)\n",
    "Y_val_predict = reg.predict(X_val)\n",
    "\n",
    "for finger in range(5):\n",
    "    ax[2*finger].plot(Y_train_predict[:,finger])\n",
    "    ax[2*finger].plot(Y_train_val[:,finger])\n",
    "    ax[2*finger].set_ylabel(f'finger {finger+1} (train)')\n",
    "    \n",
    "    ax[2*finger+1].plot(Y_val_predict[:,finger])\n",
    "    ax[2*finger+1].plot(Y_val[:,finger])\n",
    "    ax[2*finger+1].set_ylabel(f'finger {finger+1} (val)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144",
   "metadata": {},
   "source": [
    "## 2.3 Ridge Regression\n",
    "\n",
    "Ridge regession extends linear regression by adding a regularization factor `alpha` that ensures that the regression coefficients remain close to 0. \n",
    "\n",
    "The underlying idea is that higher coefficients may allow better performance on the train set to the cost of being less robust to new data. By keeping the coefficients close to 0, the model should generalize better.\n",
    "\n",
    "The regression matrix w is computed by minimizing:\n",
    "\\begin{align}\n",
    "||Y - Xw||^2 + alpha * ||w||^2\n",
    "\\end{align}\n",
    "\n",
    "The coefficient `alpha` may be optimized on the validation set. It is possible to compute a separate `alpha` value per predicted feature (here for each finger)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's evaluate 1 alpha for all fingers\n",
    "\n",
    "for i in range(11):\n",
    "    alpha = i**3\n",
    "    reg = Ridge(alpha=alpha)\n",
    "    reg.fit(X_train_val, Y_train_val)\n",
    "    print(f'alpha={alpha}:')\n",
    "    print(f'\\t train: {reg.score(X_train_val, Y_train_val)}')\n",
    "    print(f'\\t val:  {reg.score(X_val, Y_val)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's optimize a separate alpha for each finger\n",
    "\n",
    "for finger in range(Y_train.shape[1]):\n",
    "    print(f'Finger {finger}:')\n",
    "    for alpha in 0,1,100,1000,10000: # alpha=0 is basically a linear regression\n",
    "        print(f'\\t alpha={alpha}')\n",
    "        reg = Ridge(alpha=alpha)\n",
    "        reg.fit(X_train, Y_train[:,finger])\n",
    "        print(f'\\t\\t train: {reg.score(X_train, Y_train[:,finger])}')\n",
    "        print(f'\\t\\t test:  {reg.score(X_test, Y_test[:,finger])}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = np.array([1,1,1,1,1]) # Pick the best alpha for each finger\n",
    "\n",
    "reg = Ridge(alpha=best_alpha)\n",
    "reg.fit(X_train, Y_train)\n",
    "\n",
    "fig = plt.figure(figsize=(90,50))\n",
    "ax = fig.subplots(nrows=10, ncols=1)\n",
    "\n",
    "Y_train_predict = reg.predict(X_train)\n",
    "Y_test_predict = reg.predict(X_test)\n",
    "\n",
    "for finger in range(5):\n",
    "    ax[2*finger].plot(Y_train_predict[:,finger])\n",
    "    ax[2*finger].plot(Y_train[:,finger])\n",
    "    ax[2*finger].set_ylabel(f'finger {finger+1} (train)')\n",
    "    \n",
    "    ax[2*finger+1].plot(Y_test_predict[:,finger])\n",
    "    ax[2*finger+1].plot(Y_test[:,finger])\n",
    "    ax[2*finger+1].set_ylabel(f'finger {finger+1} (test)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149",
   "metadata": {},
   "source": [
    "## 1.3 Evaluation\n",
    "\n",
    "Let's pick the best model evaluated on the validation set and evaluate it on the test set.\n",
    "\n",
    "So far we used the sklearn built-in regression metrics because of it's convenience. In practice, different regression problems require different evaluations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train your best model \n",
    "\n",
    "reg = LinearRegression()\n",
    "\n",
    "reg.fit(X_train_val, Y_train_val)\n",
    "Y_train_predict = reg.predict(X_train_val)\n",
    "Y_test_predict = reg.predict(X_test)\n",
    "\n",
    "print(f'train score: {reg.score(X_train_val,Y_train_val)}')\n",
    "print(f'test score: {reg.score(X_test,Y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151",
   "metadata": {},
   "source": [
    "### MSE\n",
    "\n",
    "A very common metric is the mean-squared error, which is defined by:\n",
    "\n",
    "\\begin{align}\n",
    "mse(x,y) = \\frac{1}{n} \\sum_{i=0}^{n-1} (x_i - y_i)^2 \n",
    "\\end{align}\n",
    "\n",
    "The `mse` is derived from the euclidean distance. Due to to the square, it penalizes estimated values that are too far from the ground truth. Minimizing the `mse` is equivalent to minimizing the variance.\n",
    "\n",
    "Lower is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(f'train mse: {mean_squared_error(Y_train_predict,Y_train_val)}')\n",
    "print(f'test mse: {mean_squared_error(Y_test_predict,Y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153",
   "metadata": {},
   "outputs": [],
   "source": [
    "for finger in range(5):\n",
    "    print(f'finger {finger}:')\n",
    "    train_mse = mean_squared_error(Y_train_predict[:,finger],\n",
    "                                   Y_train_val[:,finger])\n",
    "    test_mse = mean_squared_error(Y_test_predict[:,finger],\n",
    "                                  Y_test[:,finger])\n",
    "    print(f'\\ttrain mse: {train_mse}')\n",
    "    print(f'\\ttest mse: {test_mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154",
   "metadata": {},
   "source": [
    "### Correlation\n",
    "\n",
    "The Pearson correlation measures the linear relationship between two datasets X and Y. \n",
    "\n",
    "The test returns both a correlation coefficient and a p-value. \n",
    "\n",
    "\n",
    "A coefficient of 0 means no relationship between both datasets, while a coefficient of 1 or -1 means an exact linear relationship between both datasets.\n",
    "Positive correlations imply that as x increases, so does y. Negative correlations imply that as X increases, Y decreases.\n",
    "\n",
    "The p-value indicates the probability of two uncorrelated datasets getting a correlation coefficient as good or better. In practice, it is common to set a threshold for the p-value under which the results are considered significant. Common thresholds are `0.05`, `0.01`, `0.001` with increasing confidence about the results. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "for finger in range(5):\n",
    "    print(f'finger {finger}:')\n",
    "    pearson_train = pearsonr(Y_train_predict[:,finger], \n",
    "                             Y_train_val[:,finger])\n",
    "    pearson_test = pearsonr(Y_test_predict[:,finger],\n",
    "                            Y_test[:,finger])\n",
    "    \n",
    "    print(f'\\ttrain corr: {pearson_train.statistic:.2f} (p={pearson_train.pvalue:.2E})')\n",
    "    print(f'\\ttest corr: {pearson_test.statistic:.2f} (p={pearson_test.pvalue:.2E})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tp-decoding)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
