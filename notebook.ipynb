{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift + Enter to run the current cell\n",
    "print('Hello World')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# This is an example Marakdown cell\n",
    "This is nice to write down instructions and format text with title, **bold** and _italic_\n",
    "\n",
    "You can change the cell type in the top bar menu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "#### finger \n",
    "link = https://www.bbci.de/competition/download/competition_iv/BCICIV_4_mat.zip\n",
    "eval = https://www.bbci.de/competition/iv/results/ds4/true_labels.zip\n",
    "\n",
    "#### foot tracking\n",
    "download page: http://neurotycho.org/expdatalist/listview?task=36\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Part 0 : import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "import numpy as np # library providing efficient array manipulation\n",
    "import sklearn # machine learning tools\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt # matlab-like plot library\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 22})\n",
    "\n",
    "from librosa.util import frame\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from scipy.signal import iirdesign, sosfilt, convolve, wiener\n",
    "\n",
    "import tp # load python module written for this project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Part 0 : Dataset\n",
    "\n",
    "ECoG dataset of finger tapping, more details here: https://www.bbci.de/competition/iv/desc_4.pdf\n",
    "\n",
    "### Download dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'data/bciciv'\n",
    "if not tp.dataset_exists(dataset_path):\n",
    "    tp.download_dataset(dataset_path)\n",
    "else:\n",
    "    print(f'dataset already exists in {dataset_path}') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tp.load_data(dataset_path)\n",
    "print(dataset)\n",
    "fs = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Explore dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in dataset:\n",
    "    print(subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject0 = dataset[0] # first subject in dataset\n",
    "print(subject0.train_fingers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject0.train_fingers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get first sample for all fingers, indexing starts at 0 /!\\\n",
    "subject0.train_fingers[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject0.train_fingers[-1, :] # get last sample for all fingers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject0.train_fingers[1200:1300, 4] # get 100 samples from index 1200 to 1299 for the 5th finger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "#### Plot the data with matplotlib\n",
    "You can find the documentation for the `plot` function here: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html\n",
    "\n",
    "For more information, you can check the [quick start guide](https://matplotlib.org/stable/users/explain/quick_start.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fingers = subject0.train_fingers.shape[1]\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "axes = fig.subplots(n_fingers, 1, sharex=True)\n",
    "\n",
    "for i in range(n_fingers):\n",
    "    axes[i].plot(subject0.train_fingers[:,i])\n",
    "    axes[i].set_ylabel(f'finger {i+1}')\n",
    "\n",
    "fig.suptitle('Finger trajectories (train set)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_ratio = 40\n",
    "subject_data = dataset[0]\n",
    "\n",
    "\n",
    "(n_times_train, n_fingers) = subject_data.train_fingers.shape\n",
    "n_channels = subject_data.train_ecog.shape[1]\n",
    "n_times_train = n_times_train//f_ratio\n",
    "threshold = 0.5\n",
    "n_times_test = subject_data.test_fingers.shape[0]//f_ratio\n",
    "\n",
    "win_size = 25\n",
    "win = 1/win_size*np.ones([win_size, 1])\n",
    "train_fingers_filtered = convolve(subject_data.train_fingers[::f_ratio], win, 'same')\n",
    "train_fingers_filtered = np.concatenate((train_fingers_filtered, threshold*np.ones([n_times_train, 1])), axis=1)\n",
    "y_train = np.argmax(train_fingers_filtered, axis=1)\n",
    "cue_train = np.zeros([n_times_train, n_fingers+1])\n",
    "for i in range(n_times_train):\n",
    "    cue_train[i, y_train[i]] = 1\n",
    "\n",
    "test_fingers_filtered = convolve(subject_data.test_fingers[::f_ratio], win, 'same')\n",
    "test_fingers_filtered = np.concatenate((test_fingers_filtered, threshold*np.ones([n_times_test, 1])), axis=1)\n",
    "y_test = np.argmax(test_fingers_filtered, axis=1)\n",
    "cue_test = np.zeros([n_times_test, n_fingers+1])\n",
    "for i in range(n_times_test):\n",
    "    cue_test[i, y_test[i]] = 1\n",
    "\n",
    "# Save the full data for future use\n",
    "y_train_full = y_train\n",
    "y_test_full = y_test\n",
    "\n",
    "fig = plt.figure(figsize=(150,5))\n",
    "ax = fig.subplots(3, 1)\n",
    "for i in range(n_fingers):\n",
    "    ax[0].plot(subject_data.train_fingers[::f_ratio, i])\n",
    "    ax[1].plot(train_fingers_filtered[:, i])\n",
    "    ax[2].plot(cue_train[:, i])\n",
    "ax[1].plot([0, n_times_train], [threshold, threshold], color='k')\n",
    "fig.suptitle('Finger trajectories (train set)')\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(150,5))\n",
    "ax = fig.subplots(3, 1)\n",
    "for i in range(n_fingers):\n",
    "    ax[0].plot(subject_data.test_fingers[::f_ratio, i])\n",
    "    ax[1].plot(test_fingers_filtered[:, i])\n",
    "    ax[2].plot(cue_test[:, i])\n",
    "ax[1].plot([0, n_times_test], [threshold, threshold], color='k')\n",
    "fig.suptitle('Finger trajectories (test set)')\n",
    "\n",
    "# click on the figure to expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(50,5))\n",
    "ax = fig.subplots()\n",
    "\n",
    "ax.plot(subject_data.train_ecog[:, 0])\n",
    "\n",
    "fig.suptitle('First ecog channel (train set)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "# Part 1 : Dicrete decoding\n",
    "\n",
    "In this part we will be interested in decoding finger mouvement during tapping from ECOG data and in order to do so we will focus on start with feature extraction by extracting time-frequency features on every electrodes using short term fourier transfrom. \n",
    "\n",
    "After this we will get a first set of results that we will visualize using different metrics such as the confusion matrix, the accuracy and the balanced accuracy. \n",
    "\n",
    "Then we will have an example of the usefullness of a validation set in the case of lagged responses. \n",
    "\n",
    "Once the lag have been set, we will see that we can also get better results using spatial filters such as PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 1.1 Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Temporal filtering\n",
    "\n",
    "In this part we will focus on the filtering of the data using classical filtering function such as `iirdesign` and `sosfilt` from `scipy.signal` (https://docs.scipy.org/doc/scipy/reference/signal.html). \n",
    "It is pretty common in ECOG to filter in several frequency bands and use all the resulting features for the decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bands = [(1, 60), (60, 100), (100, 200)]\n",
    "# bands = [(1, 10)]\n",
    "# for i in range(1, 20):\n",
    "#     bands.append((i*10, (i+1)*10))\n",
    "filters = []\n",
    "\n",
    "bands = [(1, 10), (10,30), (30,50), (70,200)]\n",
    "n_filters = len(bands)\n",
    "\n",
    "for i_band, band in enumerate(bands):\n",
    "   # filters.append(iirdesign(band[0], band[1], 1, 60, analog=False, fs=fs, output='sos'))\n",
    "    filters.append(iirdesign(band, (band[0]*0.9, band[1]*1.1), 1, 60, analog=False, fs=fs, output='sos'))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_ecog_normalized = scaler.fit_transform(subject_data.train_ecog)\n",
    "test_ecog_normalized = scaler.fit_transform(subject_data.test_ecog)\n",
    "\n",
    "train_ecog_filtered = []\n",
    "test_ecog_filtered = []\n",
    "\n",
    "for filt in filters:\n",
    "    train_ecog_filtered.append(sosfilt(filt, train_ecog_normalized, axis=0))\n",
    "    test_ecog_filtered.append(sosfilt(filt, test_ecog_normalized, axis=0))\n",
    "\n",
    "fig = plt.figure(figsize=(50,15))\n",
    "ax = fig.subplots(nrows=4, ncols=1)\n",
    "\n",
    "ax[0].plot(train_ecog_filtered[0][:1000, 0])\n",
    "ax[1].plot(train_ecog_filtered[1][:1000, 0])\n",
    "ax[2].plot(train_ecog_filtered[2][:1000, 0])\n",
    "ax[3].plot(train_ecog_filtered[3][:1000, 0])\n",
    "\n",
    "ax[0].plot(train_ecog_normalized[:1000, 0])\n",
    "ax[1].plot(train_ecog_normalized[:1000, 0])\n",
    "ax[2].plot(train_ecog_normalized[:1000, 0])\n",
    "ax[3].plot(train_ecog_normalized[:1000, 0])\n",
    "\n",
    "fig.suptitle('Filtered data (train set)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Average power computation\n",
    "\n",
    "Once the signal has been filtered, you can extract the average power of the signal in those frequency band using the `power` and `mean` function from `numpy`. Note that the formula for the average power of a centered signal $x$ is \n",
    "\n",
    "\\begin{align}\n",
    "P(x) = \\frac{1}{N}\\sum_{t}x^2(t),\n",
    "\\end{align}\n",
    "\n",
    "with $N$ the number of elements in the window.  \n",
    "\n",
    "In this part we will extract power in 1s of signal every 40ms. In order to compute the frame to be averaged you can use the fonction `frame` from `librosa.util`(https://librosa.org/doc/latest/generated/librosa.util.frame.html) and the padding function `pad` from `numpy` because for the first frames you can't take the 1s of previous signal as it doesn't exist, therefore you can replace it with zeros.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_size = 1000\n",
    "wiener_win_size = 25\n",
    "n_buffer = 1;\n",
    "\n",
    "X_train_1kHz = np.pad(np.power(np.concatenate(train_ecog_filtered, axis=1), 2), [(win_size-f_ratio, 0), (0, 0)])\n",
    "X_train = np.mean(frame(X_train_1kHz, frame_length=win_size, hop_length=f_ratio, axis=0),axis=1)\n",
    "X_train = frame(np.pad(X_train, [(n_buffer-1, 0), (0, 0)]), frame_length=n_buffer, hop_length=1, axis=0).reshape(n_times_train, n_buffer*n_channels*n_filters)\n",
    "# X_train = wiener(X_train, (1, wiener_win_size))\n",
    "X_train_full = X_train\n",
    "\n",
    "X_test_1kHz = np.pad(np.power(np.concatenate(test_ecog_filtered, axis=1), 2), [(win_size-f_ratio, 0), (0, 0)])\n",
    "X_test = np.mean(frame(X_test_1kHz, frame_length=win_size, hop_length=f_ratio, axis=0),axis=1)\n",
    "X_test = frame(np.pad(X_test, [(n_buffer-1, 0), (0, 0)]), frame_length=n_buffer, hop_length=1, axis=0).reshape(n_times_test, n_buffer*n_channels*n_filters)\n",
    "\n",
    "# X_test = wiener(X_test, (1, wiener_win_size))\n",
    "X_test_full = X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### Set if we use the idle or not\n",
    "\n",
    "In order to make things easier you can chose to select only the fingers and not the idle task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "use_idle_data = True\n",
    "if not use_idle_data:\n",
    "    X_train = X_train_full[y_train_full != 5]\n",
    "    X_test = X_test_full[y_test_full != 5]\n",
    "    y_train = y_train_full[y_train_full != 5]\n",
    "    y_test = y_test_full[y_test_full != 5]\n",
    "    n_times_train = y_train.shape[0]\n",
    "    n_times_test = y_test.shape[0]\n",
    "else:\n",
    "    X_train = X_train_full\n",
    "    X_test = X_test_full\n",
    "    y_train = y_train_full\n",
    "    y_test = y_test_full\n",
    "    n_times_train = y_train.shape[0]\n",
    "    n_times_test = y_test.shape[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## 1.2 Classification\n",
    "\n",
    "### The classifier\n",
    "\n",
    "A classifier is an algorithm that aims to classify given inputs (here the power in given frequency bands) into outputs (here our 5 possible fingers + the idle class where the user does nothing). \n",
    "\n",
    "In this particular example we will use either the `LinearDiscriminantAnalysis` (imported here as `LDA`) from `sklearn.discriminant_analysis` or the `LinearSVC` from `sklearn.svm`. \n",
    "See their documentation here : \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC\n",
    "\n",
    "In the sklearn API, all classes are using the `fit` function to train the algorithm and the `predict` function to get the predictions for the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Fit the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = LDA()\n",
    "\n",
    "estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### Get the predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = estimator.predict(X_test)\n",
    "y_proba = estimator.predict_proba(X_test)\n",
    "\n",
    "cue_pred = np.zeros([n_times_test, n_fingers+1])\n",
    "for i in range(n_times_test):\n",
    "    cue_pred[i, y_pred[i]] = 1\n",
    "\n",
    "fig = plt.figure(figsize=(150,5))\n",
    "ax = fig.subplots(3, 1)\n",
    "\n",
    "cue_test = np.zeros([n_times_test, n_fingers+1])\n",
    "for i in range(n_times_test):\n",
    "    cue_test[i, y_test[i]] = 1\n",
    "\n",
    "for i in range(n_fingers):\n",
    "    ax[0].plot(cue_test[:, i])\n",
    "    ax[1].plot(y_proba[:, i])\n",
    "    ax[2].plot(cue_pred[:, i])\n",
    "fig.suptitle('Finger trajectories (test set)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## 1.3 Confusion matrix and metrics\n",
    "\n",
    "Once you get your predictions you usually want to evaluate the performance of your decoder. In order to do so, a lot of different metrics have been developped with time. Most of the are using what is called a confusion matrix in order to be computed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "### The confusion matrix\n",
    "\n",
    "The confusion matrix represents how well our classifier is performing. The rows are the true labels we are expected to obtain and the columns are the labels our classifier actually gives us. \n",
    "\n",
    "In order to plot the confusion matrix, you can use the `from_predictions` function from the `ConfusionMatrixDisplay` class of `sklearn.metrics`.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(100,100))\n",
    "disp = ConfusionMatrixDisplay.from_predictions(y_test, y_pred) # , text_kw={'fontsize': 10}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### The accuracy\n",
    "\n",
    "A good way to consider how well the classifer is performing using the confusion matrix is the accuracy. The accuracy is equal to the total number of well predicted labels divided by the total number of labels. In the confusion matrix it is equal to the sum of the diagonal divided by the sum of all the elements of the matrix. \n",
    "\n",
    "You can get the accuracy using the `accuracy_score` function from `sklearn.metrics`. \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy is {:.2f}%. Chance is at {:.2f}%\".format(100*np.mean(acc), 100/np.unique(y_test).shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "### The balanced accuracy\n",
    "\n",
    "In some cases there are more labels from some classes than others, for example here the finger 2 and 5 are less represented than others so their accuracy accounts for less than the others when we compute the accuracy metric. A good way to consider every labels with the same weight is to compute the balanced accuracy. It gives the accuracy for each class reweighted so that every class accounts for as much as the others. \n",
    "\n",
    "It is computed using the `balanced_accuracy_score` function from `sklearn.metrics`.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html#sklearn-metrics-balanced-accuracy-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_fingers+1):\n",
    "    print(\"There are {} labels in test for class {}\".format(np.sum(y_test==i), i+1))\n",
    "\n",
    "plt.hist(y_test, bins=n_fingers+1, rwidth=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = balanced_accuracy_score(y_test, y_pred)\n",
    "print(\"Average score is {:.2f}%. Chance is at {:.2f}%\".format(100*np.mean(acc), 100/np.unique(y_test).shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "## 1.4 The validation set\n",
    "\n",
    "Sometimes algorithms and methods need to set a specific parameter to get better results. In order to do so, we usually use what we call a validation set. This set is usually a subdivision of the training set that works as a validation for the parameter we are trying to optimize. In the next example, we will try to find the best lag between the input and the output.\n",
    "\n",
    "### Use the validation to compute a lag value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = [i for i in range(-26, 26, 2)] \n",
    "X_train_val, X_val, y_train_val, y_val = train_test_split(X_train, y_train, test_size=0.33, random_state=13, shuffle=False)\n",
    "lagged_accuracies = []\n",
    "n_times_train_val = X_train_val.shape[0]\n",
    "n_times_val = X_val.shape[0]\n",
    "for i_lag, lag in enumerate(lags):\n",
    "    X_train_lagged = X_train_val[max(-lag, 0):min(n_times_train_val-lag, n_times_train_val)]\n",
    "    y_train_lagged = y_train_val[max(lag, 0):min(n_times_train_val+lag, n_times_train_val)]\n",
    "    X_val_lagged = X_val[max(-lag, 0):min(n_times_val-lag, n_times_val)]\n",
    "    y_val_lagged = y_val[max(lag, 0):min(n_times_val+lag, n_times_val)]\n",
    "    \n",
    "    estimator.fit(X_train_lagged, y_train_lagged)\n",
    "    y_pred_val = estimator.predict(X_val_lagged)\n",
    "    lagged_accuracies.append(accuracy_score(y_val_lagged, y_pred_val))\n",
    "    print(\"Lag {}/{} ({:.2f}s) -> Accuracy = {:.2f}%\".format(i_lag+1, len(lags), lag/25, lagged_accuracies[-1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "### Compute the best lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(-1, 1, len(lagged_accuracies)), lagged_accuracies)\n",
    "best_lag = lags[np.argmax(lagged_accuracies)]\n",
    "print(\"The best lag is {}. This lag correspond to {:.2f}s\".format(best_lag, best_lag/25))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "### Compute lagged X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lagged = X_train[max(-best_lag, 0):min(n_times_train-best_lag, n_times_train)]\n",
    "y_train_lagged = y_train[max(best_lag, 0):min(n_times_train+best_lag, n_times_train)]\n",
    "n_times_train_lagged = n_times_train-abs(best_lag)\n",
    "\n",
    "X_test_lagged = X_test[max(-best_lag, 0):min(n_times_test-best_lag, n_times_test)]\n",
    "y_test_lagged = y_test[max(best_lag, 0):min(n_times_test+best_lag, n_times_test)]\n",
    "n_times_test_lagged = n_times_test-abs(best_lag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "### Train the estimator and decode testing data with lagged estimates of X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(X_train_lagged, y_train_lagged)\n",
    "y_pred = estimator.predict(X_test_lagged)\n",
    "y_proba = estimator.predict_proba(X_test_lagged)\n",
    "\n",
    "cue_pred = np.zeros([n_times_test_lagged, n_fingers+1])\n",
    "for i in range(n_times_test_lagged):\n",
    "    cue_pred[i, y_pred[i]] = 1\n",
    "\n",
    "cue_test = np.zeros([n_times_test_lagged, n_fingers+1])\n",
    "for i in range(n_times_test_lagged):\n",
    "    cue_test[i, y_test[i]] = 1\n",
    "\n",
    "fig = plt.figure(figsize=(150,5))\n",
    "ax = fig.subplots(3, 1)\n",
    "for i in range(n_fingers):\n",
    "    ax[0].plot(cue_test[:, i])\n",
    "    ax[1].plot(y_proba[:, i])\n",
    "    ax[2].plot(cue_pred[:, i])\n",
    "fig.suptitle('Finger trajectories (test set)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "### Compute metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(100,100))\n",
    "disp = ConfusionMatrixDisplay.from_predictions(y_test_lagged, y_pred)\n",
    "\n",
    "acc = accuracy_score(y_test_lagged, y_pred)\n",
    "print(\"Accuracy is {:.2f}%. Chance is at {:.2f}%\".format(100*np.mean(acc), 100/np.unique(y_test).shape[0]))\n",
    "\n",
    "acc = balanced_accuracy_score(y_test_lagged, y_pred)\n",
    "print(\"Balanced accuracy is {:.2f}%. Chance is at {:.2f}%\".format(100*np.mean(acc), 100/np.unique(y_test).shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "## 1.5 Feature selection and dimensionality reduction\n",
    "\n",
    "In some cases in machine learning there are so much features that the model cannot correctly learn for the training data. In these cases, it is very common to compute dimensionnality reduction or spatial filters in order to keep only the features of interest for decoding. In this example we will use a Principal Components Analysis (the `PCA` class from `sklearn.decomposition`) and compute patterns for each finger in order to improve the results.\n",
    "\n",
    "In `sklearn`, the classes that transform data without predicting are called transformers and use the functions `fit` and `transform` to process data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca = []\n",
    "X_test_pca = []\n",
    "for i in range(n_fingers):\n",
    "    print(\"Computing PCA patterns for finger {}...\".format(i+1))\n",
    "    s_filt = PCA(60)\n",
    "    s_filt.fit(X_train_lagged[y_train_lagged==i])\n",
    "    X_train_pca.append(s_filt.transform(X_train_lagged))\n",
    "    X_test_pca.append(s_filt.transform(X_test_lagged))\n",
    "\n",
    "X_train_pca = np.concatenate(X_train_pca, axis=1)\n",
    "X_test_pca = np.concatenate(X_test_pca, axis=1)\n",
    "\n",
    "estimator.fit(X_train_pca, y_train_lagged)\n",
    "y_pred_pca = estimator.predict(X_test_pca)\n",
    "y_proba_pca = estimator.predict_proba(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "cue_pred = np.zeros([n_times_test_lagged, n_fingers+1])\n",
    "for i in range(n_times_test_lagged):\n",
    "    cue_pred[i, y_pred_pca[i]] = 1\n",
    "\n",
    "cue_test = np.zeros([n_times_test_lagged, n_fingers+1])\n",
    "for i in range(n_times_test_lagged):\n",
    "    cue_test[i, y_test[i]] = 1\n",
    "\n",
    "fig = plt.figure(figsize=(150,5))\n",
    "ax = fig.subplots(3, 1)\n",
    "for i in range(n_fingers):\n",
    "    ax[0].plot(cue_test[:, i])\n",
    "    ax[1].plot(y_proba_pca[:, i])\n",
    "    ax[2].plot(cue_pred[:, i])\n",
    "fig.suptitle('Finger trajectories (test set)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(100,100))\n",
    "disp = ConfusionMatrixDisplay.from_predictions(y_test_lagged, y_pred_pca)\n",
    "\n",
    "\n",
    "acc = accuracy_score(y_test_lagged, y_pred_pca)\n",
    "print(\"Accuracy is {:.2f}%. Chance is at {:.2f}%\".format(100*np.mean(acc), 100/np.unique(y_test).shape[0]))\n",
    "\n",
    "acc = balanced_accuracy_score(y_test_lagged, y_pred_pca)\n",
    "print(\"Balanced accuracy is {:.2f}%. Chance is at {:.2f}%\".format(100*np.mean(acc), 100/np.unique(y_test).shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "## 1.6 Get a more stable output (optional)\n",
    "\n",
    "On decoders that are to be used by humans, it is important to get a more stable output so that it doesn't radically change every 40ms for example. This does not always improve classification but it can improve the feeling of control for the user with the side effect of adding a bit of lag. \n",
    "\n",
    "In this way, we can get the output probability of the decoder using the `predict_proba` function and smooth it using a convolution with a window. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_size = 25\n",
    "win = np.expand_dims(scipy.signal.windows.hamming(win_size), 1)\n",
    "y_proba_stable = convolve(y_proba_pca, win, 'same')\n",
    "y_pred_stable = np.argmax(y_proba_stable, axis=1)\n",
    "\n",
    "cue_pred = np.zeros([n_times_test_lagged, n_fingers+1])\n",
    "for i in range(n_times_test_lagged):\n",
    "    cue_pred[i, y_pred_stable[i]] = 1\n",
    "\n",
    "cue_test = np.zeros([n_times_test_lagged, n_fingers+1])\n",
    "for i in range(n_times_test_lagged):\n",
    "    cue_test[i, y_test[i]] = 1\n",
    "\n",
    "fig = plt.figure(figsize=(150,5))\n",
    "ax = fig.subplots(3, 1)\n",
    "for i in range(n_fingers):\n",
    "    ax[0].plot(cue_test[:, i])\n",
    "    ax[1].plot(y_proba_stable[:, i])\n",
    "    ax[2].plot(cue_pred[:, i])\n",
    "fig.suptitle('Finger trajectories (test set)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "### Get metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(100,100))\n",
    "disp = ConfusionMatrixDisplay.from_predictions(y_test_lagged, y_pred_stable)\n",
    "\n",
    "acc = accuracy_score(y_test_lagged, y_pred_stable)\n",
    "print(\"Accuracy is {:.2f}%. Chance is at {:.2f}%\".format(100*np.mean(acc), 100/np.unique(y_test).shape[0]))\n",
    "\n",
    "acc = balanced_accuracy_score(y_test_lagged, y_pred_stable)\n",
    "print(\"Balanced accuracy is {:.2f}%. Chance is at {:.2f}%\".format(100*np.mean(acc), 100/6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "# Part 2 : Continuous decoding\n",
    "\n",
    "In this part we will be interested in decoding precise hand movements of a monkey from ECOG data and in order to do so we will procede just as in the Part 1 with\n",
    "- Pre-processing - where we will see artefact removal for example\n",
    "- Feature extraction - by extracting time-frequency features on every electrodes using short term fourier transfrom\n",
    "- Feature reduction using PCA\n",
    "\n",
    "After this first part we will focus on classification paradigms with\n",
    "- Cross validation in order to ensure the robustness of our testing procedure\n",
    "- Regression using a linear model \n",
    "\n",
    "Finally we will evaluate the model performances using mean square error or correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "## 1.1 Data pre-processing\n",
    "\n",
    "### Artefact removal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "### Feature selection and dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "## 1.2 Classification\n",
    "\n",
    "### Train-test and cross-validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "### Get the predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "### Using the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "## 1.3 Metrics for continuous decoding\n",
    "\n",
    "### RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tp-decoding)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
